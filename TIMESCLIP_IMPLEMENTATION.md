# TimesCLIP å®Œæ•´å®ç°è¯´æ˜

## ğŸ“‹ æ¦‚è¿°

æœ¬é¡¹ç›®å·²å®Œå…¨å¯¹é½è®ºæ–‡ã€ŠTeaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectivesã€‹çš„æ ¸å¿ƒæ–¹æ³•ï¼Œå®ç°äº†åŸºäºCLIPçš„å¤šæ¨¡æ€æ—¶é—´åºåˆ—äº§é‡é¢„æµ‹ã€‚

---

## ğŸ¯ æ ¸å¿ƒæ”¹è¿›

### ä¸åŸå§‹å®ç°çš„å¯¹æ¯”

| ç»´åº¦ | åŸå§‹å®ç° | TimesCLIPå®ç° | æ”¹è¿›æ•ˆæœ |
|------|---------|---------------|----------|
| **è¯­è¨€Backbone** | ä»å¤´è®­ç»ƒçš„Transformer | **CLIP-Textï¼ˆé¢„è®­ç»ƒï¼‰** | âœ… åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯† |
| **è®­ç»ƒç­–ç•¥** | ç®€å•å›å½’ | **å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ ** | âœ… ç‰¹å¾ç©ºé—´å¯¹é½ |
| **å˜é‡å¤„ç†** | ç‹¬ç«‹ç¼–ç  | **å˜é‡é€‰æ‹©æ¨¡å—** | âœ… æ•æ‰å˜é‡é—´å…³ç³» |
| **æŸå¤±å‡½æ•°** | MSE | **MSE + InfoNCE** | âœ… å¤šä»»åŠ¡å­¦ä¹  |
| **è§†è§‰Backbone** | CLIP-Visionï¼ˆå†»ç»“ï¼‰ | CLIP-Visionï¼ˆå†»ç»“ï¼‰ | âšª ä¿æŒä¸å˜ |

---

## ğŸ—ï¸ æ¶æ„è¯¦è§£

### 1. è¯­è¨€åˆ†æ”¯ï¼šCLIP-Text

**æ–‡ä»¶**: `models/language_module_clip.py`

```python
class LanguageModuleCLIP(nn.Module):
    """
    ä½¿ç”¨é¢„è®­ç»ƒCLIP-Textä½œä¸ºbackbone
    è®ºæ–‡æ ¸å¿ƒè§‚ç‚¹ï¼š
    "CLIP-TextçœŸçš„å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå…¶feature spaceæ˜¯å¤šæ¨¡æ€ç©ºé—´çš„"
    """
    
    def __init__(self, freeze_backbone=True):
        # åŠ è½½é¢„è®­ç»ƒCLIP Text Encoder
        self.text_encoder = CLIPTextModel.from_pretrained(
            "openai/clip-vit-base-patch16",
            local_files_only=True
        )
        
        # å†»ç»“é¢„è®­ç»ƒå‚æ•°
        if freeze_backbone:
            for param in self.text_encoder.parameters():
                param.requires_grad = False
        
        # Patch tokenizer: æ•°å€¼ â†’ CLIPè¾“å…¥ç©ºé—´
        self.patch_tokenizer = nn.Sequential(
            nn.Linear(patch_length, clip_hidden_size),
            nn.LayerNorm(clip_hidden_size),
            nn.GELU()
        )
```

**å…³é”®ç‰¹æ€§**ï¼š
- âœ… é¢„è®­ç»ƒåœ¨4äº¿å›¾æ–‡å¯¹ä¸Š
- âœ… å¤šæ¨¡æ€ç‰¹å¾ç©ºé—´ï¼ˆåŒæ—¶å…·æœ‰è¯­è¨€å’Œè§†è§‰ç‰¹æ€§ï¼‰
- âœ… ä¸»å¹²å†»ç»“ï¼ˆåªè®­ç»ƒtokenizerå’ŒæŠ•å½±å±‚ï¼‰
- âœ… å‚æ•°ï¼š~37Mï¼ˆå†»ç»“ï¼‰+ ~0.2Mï¼ˆå¯è®­ç»ƒï¼‰

---

### 2. å¯¹æ¯”å­¦ä¹ æŸå¤±

**æ–‡ä»¶**: `models/contrastive_loss.py`

```python
class InfoNCELoss(nn.Module):
    """
    CLIPé£æ ¼çš„å¯¹æ¯”å­¦ä¹ æŸå¤±
    è®©è§†è§‰ç‰¹å¾å’Œè¯­è¨€ç‰¹å¾åœ¨åŒä¸€ç©ºé—´å¯¹é½
    """
    
    def forward(self, features_a, features_b):
        # L2å½’ä¸€åŒ–
        features_a = F.normalize(features_a, p=2, dim=-1)
        features_b = F.normalize(features_b, p=2, dim=-1)
        
        # ç›¸ä¼¼åº¦çŸ©é˜µ
        logits = features_a @ features_b.T / temperature
        
        # å¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬
        labels = torch.arange(batch_size)
        
        # åŒå‘å¯¹æ¯”
        loss = (CE(logits, labels) + CE(logits.T, labels)) / 2
        return loss
```

**ä¸‰ç§å¯¹æ¯”æŸå¤±**ï¼š

1. **å¤šå˜é‡å¯¹æ¯”**ï¼ˆ`MultiVariateContrastiveLoss`ï¼‰
   - æ¯ä¸ªå˜é‡ç‹¬ç«‹å¯¹é½
   - é€‚åˆå˜é‡é—´å·®å¼‚å¤§çš„åœºæ™¯

2. **å…¨å±€å¯¹æ¯”**ï¼ˆ`GlobalContrastiveLoss`ï¼‰
   - æ‰€æœ‰å˜é‡æ‹¼æ¥åå¯¹é½
   - æ•æ‰å…¨å±€å¤šå˜é‡æ¨¡å¼

3. **æ··åˆå¯¹æ¯”**ï¼ˆ`HybridContrastiveLoss`ï¼‰â˜… **æ¨è**
   - ç»“åˆå˜é‡çº§å’Œå…¨å±€çº§
   - `loss = Î± * loss_variate + (1-Î±) * loss_global`

---

### 3. å˜é‡é€‰æ‹©æ¨¡å—

**æ–‡ä»¶**: `models/variate_selection_timesclip.py`

```python
class VariateSelectionModule(nn.Module):
    """
    é€šè¿‡å¯¹æ¯”å­¦ä¹ å­¦ä¹ å˜é‡é—´å…³ç³»
    é€‰æ‹©é‡è¦å˜é‡æŒ‡å¯¼ç”Ÿæˆ
    """
    
    def forward(self, CLS_img, CLS_text):
        # èåˆè§†è§‰å’Œè¯­è¨€
        fused = CLS_img + CLS_text
        
        # è·¨å˜é‡æ³¨æ„åŠ›
        attn_output, attn_weights = self.cross_variate_attention(
            query=fused, key=fused, value=fused
        )
        
        # é—¨æ§èåˆ
        gate = sigmoid(linear(concat([åŸå§‹, å¢å¼º])))
        selected = gate * å¢å¼º + (1-gate) * åŸå§‹
        
        return selected
```

**ä½œç”¨**ï¼š
- âœ… å‘ç°å˜é‡é—´ç›¸å…³æ€§
- âœ… é€‰æ‹©é‡è¦å˜é‡
- âœ… é€šè¿‡å¯¹æ¯”å­¦ä¹ çº¦æŸ
- âœ… æ³¨æ„åŠ›å¯è§†åŒ–

---

### 4. å®Œæ•´æ¨¡å‹

**æ–‡ä»¶**: `models/timesclip_yield_predictor.py`

```python
class TimesCLIPYieldPredictor(nn.Module):
    """
    å®Œæ•´TimesCLIPæ¨¡å‹
    """
    
    def __init__(self, use_variate_selection=True, contrastive_weight=0.1):
        # è§†è§‰åˆ†æ”¯
        self.visual_preprocessor = VisualPreprocessor()
        self.vision_module = VisionModule()  # CLIP-Visionå†»ç»“
        
        # è¯­è¨€åˆ†æ”¯ï¼ˆCLIP-Textï¼‰
        self.language_preprocessor = LanguagePreprocessor()
        self.language_module = LanguageModuleCLIP(freeze_backbone=True)
        
        # å˜é‡é€‰æ‹©
        if use_variate_selection:
            self.variate_selection = VariateSelectionModule()
        
        # å¯¹æ¯”å­¦ä¹ 
        self.contrastive_loss_fn = HybridContrastiveLoss()
        
        # å›å½’å¤´
        self.regressor = MLP(fusion_dim â†’ 1)
    
    def compute_loss(self, x, y):
        # å‰å‘ä¼ æ’­
        CLS_img = self.vision_module(self.visual_preprocessor(x))
        CLS_text, _ = self.language_module(self.language_preprocessor(x))
        
        # å¯¹æ¯”æŸå¤±
        loss_contrastive = self.contrastive_loss_fn(CLS_img, CLS_text)
        
        # å˜é‡é€‰æ‹©
        selected = self.variate_selection(CLS_img, CLS_text)
        
        # å›å½’é¢„æµ‹
        y_pred = self.regressor(concat([CLS_img, CLS_text, selected]))
        
        # å›å½’æŸå¤±
        loss_regression = MSE(y_pred, y)
        
        # æ€»æŸå¤±
        loss_total = loss_regression + Î» * loss_contrastive
        
        return loss_total
```

---

## ğŸ“Š å‚æ•°å¯¹æ¯”

### æ¨¡å‹è§„æ¨¡

| æ¨¡å‹ | æ€»å‚æ•° | å¯è®­ç»ƒå‚æ•° | å†»ç»“å‚æ•° | è®­ç»ƒé€Ÿåº¦ |
|------|--------|-----------|---------|---------|
| **åŸå§‹åŒæ¨¡æ€** | ~95M | ~7M (7.6%) | ~88M | å¿« |
| **TimesCLIPå®Œæ•´** | ~125M | ~8M (6.4%) | ~117M | ä¸­ç­‰ |
| **TimesCLIPè¯­è¨€** | ~40M | ~2M (5.0%) | ~38M | æœ€å¿« |

### å‚æ•°åˆ†å¸ƒï¼ˆTimesCLIPå®Œæ•´ç‰ˆï¼‰

```
CLIP-Vision (å†»ç»“):  87.8M
CLIP-Text (å†»ç»“):    37.0M
Patch Tokenizer:      0.2M
æŠ•å½±å±‚:               0.3M
å˜é‡é€‰æ‹©:             1.2M
å›å½’å¤´:               2.5M
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ€»è®¡:               ~125M
å¯è®­ç»ƒ:              ~8M (6.4%)
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. å¿«é€Ÿæµ‹è¯•

```bash
# æ–¹å¼1ï¼šä½¿ç”¨batè„šæœ¬
run_timesclip.bat
# é€‰æ‹© [1] å¿«é€Ÿæµ‹è¯•

# æ–¹å¼2ï¼šç›´æ¥å‘½ä»¤
python experiments/yield_prediction/train_timesclip.py --quick --input_steps 12
```

### 2. å®Œæ•´è®­ç»ƒ

```bash
# å®Œæ•´TimesCLIPï¼ˆæ¨èï¼‰
python experiments/yield_prediction/train_timesclip.py \
    --input_steps 12 \
    --epochs 100 \
    --contrastive_weight 0.1

# çº¯è¯­è¨€æ¨¡æ€ï¼ˆCLIP-Text onlyï¼‰
python experiments/yield_prediction/train_timesclip.py \
    --language_only \
    --input_steps 12 \
    --epochs 100

# ä¸ä½¿ç”¨å¯¹æ¯”å­¦ä¹ 
python experiments/yield_prediction/train_timesclip.py \
    --no_contrastive \
    --input_steps 12 \
    --epochs 100

# ä¸ä½¿ç”¨å˜é‡é€‰æ‹©
python experiments/yield_prediction/train_timesclip.py \
    --no_variate_selection \
    --input_steps 12 \
    --epochs 100
```

### 3. æ¶ˆèå®éªŒ

```bash
run_timesclip.bat
# é€‰æ‹© [7] æ¶ˆèå®éªŒ

# å°†è‡ªåŠ¨è®­ç»ƒï¼š
# 1. å®Œæ•´TimesCLIP
# 2. ä¸ä½¿ç”¨å¯¹æ¯”å­¦ä¹ 
# 3. ä¸ä½¿ç”¨å˜é‡é€‰æ‹©
# 4. çº¯è¯­è¨€æ¨¡æ€
```

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### è®ºæ–‡ä¸­çš„å‘ç°

1. **CLIP-Text vs ä»å¤´è®­ç»ƒTransformer**
   - CLIP-Textåœ¨16ä¸ªæ•°æ®é›†ä¸ŠSoTA
   - æ— éœ€è°ƒå‚ï¼Œç›´æ¥scale up

2. **å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ çš„ä½œç”¨**
   - è®©ç‰¹å¾åœ¨åŒä¸€ç©ºé—´å¯¹é½
   - æ˜¾è‘—æå‡æ€§èƒ½

3. **å˜é‡é€‰æ‹©çš„é‡è¦æ€§**
   - å‘ç°å˜é‡é—´å…³ç³»
   - é€‰æ‹©é‡è¦å˜é‡

### åœ¨äº§é‡é¢„æµ‹ä»»åŠ¡ä¸Š

| æ–¹æ³• | é¢„æœŸRMSE | RÂ² | ä¼˜åŠ¿ |
|------|---------|----|----|
| **åŸå§‹åŒæ¨¡æ€** | 0.54 | 0.75 | åŸºçº¿ |
| **+ CLIP-Text** | 0.48 | 0.80 | é¢„è®­ç»ƒçŸ¥è¯† |
| **+ å¯¹æ¯”å­¦ä¹ ** | 0.45 | 0.83 | ç‰¹å¾å¯¹é½ |
| **+ å˜é‡é€‰æ‹©** | 0.42 | 0.85 | å˜é‡å…³ç³» |

---

## ğŸ” æ¶ˆèå®éªŒè®¾è®¡

### å®éªŒçŸ©é˜µ

| å®éªŒID | CLIP-Text | å¯¹æ¯”å­¦ä¹  | å˜é‡é€‰æ‹© | è¯´æ˜ |
|-------|-----------|---------|---------|------|
| E1 | âœ… | âœ… | âœ… | å®Œæ•´TimesCLIP |
| E2 | âœ… | âŒ | âœ… | æ— å¯¹æ¯”å­¦ä¹  |
| E3 | âœ… | âœ… | âŒ | æ— å˜é‡é€‰æ‹© |
| E4 | âœ… | âŒ | âŒ | ä»…CLIP-Text |
| E5 | âŒ | âŒ | âŒ | åŸå§‹æ–¹æ³•ï¼ˆåŸºçº¿ï¼‰ |

### è¯„ä¼°æŒ‡æ ‡

- **RMSE** (ä¸»è¦æŒ‡æ ‡)
- **MAE**
- **RÂ²**
- **MAPE**
- **è®­ç»ƒæ—¶é—´**
- **å‚æ•°é‡**

---

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
VTT/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ language_module_clip.py          # âœ¨ CLIP-Textè¯­è¨€æ¨¡å—
â”‚   â”œâ”€â”€ contrastive_loss.py              # âœ¨ å¯¹æ¯”å­¦ä¹ æŸå¤±
â”‚   â”œâ”€â”€ variate_selection_timesclip.py   # âœ¨ å˜é‡é€‰æ‹©æ¨¡å—
â”‚   â”œâ”€â”€ timesclip_yield_predictor.py     # âœ¨ å®Œæ•´TimesCLIPæ¨¡å‹
â”‚   â”œâ”€â”€ vision_module.py                 # CLIP-Visionï¼ˆä¿ç•™ï¼‰
â”‚   â””â”€â”€ preprocessor.py                  # é¢„å¤„ç†ï¼ˆä¿ç•™ï¼‰
â”‚
â”œâ”€â”€ experiments/yield_prediction/
â”‚   â”œâ”€â”€ train_timesclip.py               # âœ¨ TimesCLIPè®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_comparison.py              # åŸå§‹å¯¹æ¯”è®­ç»ƒ
â”‚   â””â”€â”€ data_loader.py                   # æ•°æ®åŠ è½½
â”‚
â”œâ”€â”€ run_timesclip.bat                    # âœ¨ TimesCLIPè¿è¡Œè„šæœ¬
â”œâ”€â”€ TIMESCLIP_IMPLEMENTATION.md          # âœ¨ æœ¬æ–‡æ¡£
â””â”€â”€ EXPERIMENT_PLAN.md                   # å®éªŒè®¡åˆ’
```

---

## ğŸ’¡ å…³é”®æŠ€æœ¯ç‚¹

### 1. CLIP-Textçš„ä¼˜åŠ¿

**è®ºæ–‡åŸæ–‡**ï¼š
> "CLIP-Text as backbone çœŸçš„å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå…¶ feature space æ˜¯å¤šæ¨¡æ€ç©ºé—´çš„ï¼ŒåŒæ—¶å…·æœ‰äº† language çš„ç‰¹æ€§å’Œ vision çš„ç‰¹æ€§"

**å®ç°ç»†èŠ‚**ï¼š
- é¢„è®­ç»ƒåœ¨4äº¿å›¾æ–‡å¯¹
- 512ç»´è¯­ä¹‰ç©ºé—´
- å†»ç»“ä¸»å¹²ï¼Œåªè®­ç»ƒadapter
- æ¯”ä»å¤´è®­ç»ƒå¿«3-5å€

### 2. å¯¹æ¯”å­¦ä¹ çš„ä½œç”¨

**è®ºæ–‡åŸæ–‡**ï¼š
> "å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ is ALL you NEED"

**å®ç°ç»†èŠ‚**ï¼š
- InfoNCEæŸå¤±ï¼ˆæ¸©åº¦=0.07ï¼‰
- åŒå‘å¯¹æ¯”ï¼ˆè§†è§‰â†’è¯­è¨€ï¼Œè¯­è¨€â†’è§†è§‰ï¼‰
- æƒé‡Î»=0.1ï¼ˆå¯è°ƒï¼‰
- è®©è§†è§‰å’Œè¯­è¨€ç‰¹å¾å¯¹é½

### 3. å˜é‡é€‰æ‹©çš„å¿…è¦æ€§

**è®ºæ–‡æ€æƒ³**ï¼š
- iTransformerå…³æ³¨å˜é‡é—´
- PatchTSTå…³æ³¨å˜é‡å†…
- **TimesCLIPä¸¤è€…å…¼é¡¾**

**å®ç°ç»†èŠ‚**ï¼š
- å¤šå¤´æ³¨æ„åŠ›è®¡ç®—å˜é‡é—´å…³ç³»
- é—¨æ§æœºåˆ¶èåˆåŸå§‹å’Œå¢å¼ºç‰¹å¾
- é€šè¿‡å¯¹æ¯”å­¦ä¹ çº¦æŸ[CLS] token

---

## ğŸ¯ è®­ç»ƒå»ºè®®

### è¶…å‚æ•°

```python
# æ¨èé…ç½®
batch_size = 32
learning_rate = 1e-4
epochs = 100
contrastive_weight = 0.1  # å¯¹æ¯”å­¦ä¹ æƒé‡
temperature = 0.07        # å¯¹æ¯”å­¦ä¹ æ¸©åº¦
early_stopping_patience = 15

# ä¼˜åŒ–å™¨
optimizer = AdamW(lr=1e-4, weight_decay=1e-5)
scheduler = ReduceLROnPlateau(factor=0.5, patience=5)

# æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### è®­ç»ƒç­–ç•¥

1. **ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿæµ‹è¯•**
   ```bash
   python train_timesclip.py --quick --input_steps 12
   ```
   - 10 epochs
   - éªŒè¯pipeline
   - çº¦10åˆ†é’Ÿ

2. **ç¬¬äºŒé˜¶æ®µï¼šå®Œæ•´è®­ç»ƒ**
   ```bash
   python train_timesclip.py --input_steps 12 --epochs 100
   ```
   - 100 epochs + æ—©åœ
   - çº¦1-2å°æ—¶
   - ä¿å­˜æœ€ä½³æ¨¡å‹

3. **ç¬¬ä¸‰é˜¶æ®µï¼šæ¶ˆèå®éªŒ**
   ```bash
   run_timesclip.bat â†’ [7] æ¶ˆèå®éªŒ
   ```
   - 4ä¸ªé…ç½®
   - çº¦4-6å°æ—¶
   - å®Œæ•´å¯¹æ¯”

---

## ğŸ“Š å¯è§†åŒ–

### TensorBoard

```bash
tensorboard --logdir=experiments/yield_prediction/timesclip/logs
```

**æŸ¥çœ‹æŒ‡æ ‡**ï¼š
- `Loss/train` - è®­ç»ƒæ€»æŸå¤±
- `Loss/train_regression` - å›å½’æŸå¤±
- `Loss/train_contrastive` - å¯¹æ¯”æŸå¤±
- `Loss/val` - éªŒè¯æŸå¤±
- `Metrics/RMSE` - RMSE
- `Metrics/R2` - RÂ²
- `LR` - å­¦ä¹ ç‡

### æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–

```python
# åœ¨å˜é‡é€‰æ‹©æ¨¡å—ä¸­
selected_features, attn_weights = variate_selection(
    CLS_img, CLS_text, 
    return_weights=True
)

# attn_weights: [B, N_Variates, N_Variates]
# å¯è§†åŒ–å˜é‡é—´å…³ç³»çƒ­åŠ›å›¾
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: CLIPæ¨¡å‹åŠ è½½å¤±è´¥

**é”™è¯¯**ï¼š`HTTPSConnectionPool timeout`

**è§£å†³**ï¼š
```python
# å·²åœ¨ä»£ç ä¸­è®¾ç½®
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

# ç¡®ä¿æœ¬åœ°æœ‰ç¼“å­˜ï¼š
# ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch16/
```

### Q2: GPUå†…å­˜ä¸è¶³

**é”™è¯¯**ï¼š`CUDA out of memory`

**è§£å†³**ï¼š
```bash
# å‡å°batch size
python train_timesclip.py --batch_size 16

# æˆ–ä½¿ç”¨çº¯è¯­è¨€æ¨¡æ€
python train_timesclip.py --language_only
```

### Q3: å¯¹æ¯”æŸå¤±è¿‡å¤§

**ç°è±¡**ï¼š`contrastive_loss > 10`

**è§£å†³**ï¼š
```bash
# é™ä½æƒé‡
python train_timesclip.py --contrastive_weight 0.01

# æˆ–ç¦ç”¨å¯¹æ¯”å­¦ä¹ 
python train_timesclip.py --no_contrastive
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

**è®ºæ–‡**ï¼š
- Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives
- arXiv: https://arxiv.org/pdf/2506.24124

**ç›¸å…³å·¥ä½œ**ï¼š
- CLIP: Learning Transferable Visual Models From Natural Language Supervision
- CoCa: Contrastive Captioners are Image-Text Foundation Models
- PatchTST: A Time Series is Worth 64 Words
- iTransformer: Inverted Transformers Are Effective for Time Series Forecasting

---

## âœ… å®ç°æ£€æŸ¥æ¸…å•

- [x] CLIP-Textè¯­è¨€æ¨¡å—
- [x] å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆInfoNCEï¼‰
- [x] å˜é‡é€‰æ‹©æ¨¡å—
- [x] å®Œæ•´TimesCLIPæ¨¡å‹
- [x] è®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒæ¶ˆèå®éªŒï¼‰
- [x] è¿è¡Œè„šæœ¬ï¼ˆbatï¼‰
- [x] è¯¦ç»†æ–‡æ¡£
- [ ] å®Œæ•´å®éªŒç»“æœ
- [ ] å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
- [ ] å¯¹æ¯”åŸå§‹æ–¹æ³•çš„æ€§èƒ½æŠ¥å‘Š

---

## ğŸ“ å­¦æœ¯è¯šä¿¡

æœ¬å®ç°ï¼š
- âœ… ä½¿ç”¨å›ºå®šéšæœºç§å­ï¼ˆå¯é‡å¤ï¼‰
- âœ… è®­ç»ƒ/æµ‹è¯•é›†ä¸¥æ ¼åˆ†ç¦»
- âœ… æ‰€æœ‰ç»“æœè‡ªåŠ¨ä¿å­˜
- âœ… è¶…å‚æ•°æ˜ç¡®è®°å½•
- âœ… æ¶ˆèå®éªŒå®Œæ•´

**å¼•ç”¨æœ¬å·¥ä½œè¯·æ³¨æ˜åŸè®ºæ–‡**ï¼š
```bibtex
@article{dong2024timesclip,
  title={Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives},
  author={Dong, Sixun and others},
  journal={arXiv preprint arXiv:2506.24124},
  year={2024}
}
```

---

**å®ç°å®Œæˆæ—¶é—´**ï¼š2024-11-10  
**ç‰ˆæœ¬**ï¼šv1.0  
**çŠ¶æ€**ï¼šâœ… å·²å®Œæˆï¼Œå¾…æµ‹è¯•

