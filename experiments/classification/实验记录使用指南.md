# å®éªŒè®°å½•ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## ğŸ¯ ç›®æ ‡

å»ºç«‹ä¸€å¥—å®Œæ•´çš„å®éªŒè¿½è¸ªç³»ç»Ÿï¼Œæ–¹ä¾¿ï¼š
1. âœ… è®°å½•æ¯æ¬¡å®éªŒçš„é…ç½®å’Œç»“æœ
2. âœ… å¯¹æ¯”ä¸åŒé…ç½®çš„æ•ˆæœ
3. âœ… ç”Ÿæˆè®ºæ–‡æ‰€éœ€çš„è¡¨æ ¼å’Œå›¾è¡¨
4. âœ… å¿«é€Ÿå›é¡¾å†å²å®éªŒ

---

## ğŸ“‹ æ ¸å¿ƒæ–‡ä»¶

| æ–‡ä»¶ | ä½œç”¨ |
|------|------|
| `experiment_tracker.py` | å®éªŒè¿½è¸ªæ ¸å¿ƒæ¨¡å— |
| `view_experiments.py` | æŸ¥çœ‹å®éªŒç»“æœçš„ä¾¿æ·è„šæœ¬ |
| `EXPERIMENT_PROTOCOL.md` | å®éªŒè®°å½•è§„èŒƒå’Œæ¨¡æ¿ |
| `experiment_logs/experiments.csv` | æ‰€æœ‰å®éªŒçš„ä¸»è®°å½•è¡¨ |
| `experiment_logs/exp_XXX_detail.json` | æ¯ä¸ªå®éªŒçš„è¯¦ç»†é…ç½® |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è¿è¡Œè®­ç»ƒï¼ˆè‡ªåŠ¨è®°å½•ï¼‰

```bash
cd experiments/classification
python train_12steps_dual_cached.py
```

è®­ç»ƒç»“æŸåä¼š**è‡ªåŠ¨è®°å½•**åˆ°å®éªŒè¿½è¸ªç³»ç»Ÿã€‚

### 2. æŸ¥çœ‹å®éªŒæ‘˜è¦

```bash
python view_experiments.py
# æˆ–
python view_experiments.py --summary
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
======================================================================
å®éªŒè¿½è¸ªæ‘˜è¦
======================================================================
æ€»å®éªŒæ•°: 5
æœ€é«˜Val F1: 0.6800 (å®éªŒ: exp_20251122_150000)
å¹³å‡Val F1: 0.6120
å¹³å‡è¿‡æ‹Ÿåˆå·®è·: 0.1820
======================================================================
```

### 3. æŸ¥çœ‹Top 3å®éªŒ

```bash
python view_experiments.py --top 3
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
======================================================================
Top 3 å®éªŒï¼ˆæŒ‰Val F1æ’åºï¼‰
======================================================================

ğŸ¥‡ æ’å 1: exp_20251122_150000
   æè¿°: Heavyæ•°æ®å¢å¼º + Dropout=0.3
   å¢å¼º: heavy | Dropout: 0.3 | WD: 0.0005
   Val F1: 0.6800 | Test F1: 0.6750
   è¿‡æ‹Ÿåˆ: 0.0400 (Train F1 - Val F1)
   å„ç±»F1: [0.600, 0.680, 0.720, 0.750]

ğŸ¥ˆ æ’å 2: exp_20251122_143000
   æè¿°: Mediumæ•°æ®å¢å¼º + Dropout=0.3
   å¢å¼º: medium | Dropout: 0.3 | WD: 0.0005
   Val F1: 0.6500 | Test F1: 0.6450
   è¿‡æ‹Ÿåˆ: 0.1300 (Train F1 - Val F1)
   å„ç±»F1: [0.550, 0.650, 0.680, 0.700]

ğŸ¥‰ æ’å 3: exp_20251122_120000
   æè¿°: Baseline - æ— æ•°æ®å¢å¼º
   å¢å¼º: none | Dropout: 0.1 | WD: 0.0001
   Val F1: 0.5630 | Test F1: 0.5500
   è¿‡æ‹Ÿåˆ: 0.3720 (Train F1 - Val F1)
   å„ç±»F1: [0.450, 0.580, 0.600, 0.620]
======================================================================
```

### 4. å¯¹æ¯”æ•°æ®å¢å¼ºæ•ˆæœ

```bash
python view_experiments.py --augmentation
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
======================================================================
æ•°æ®å¢å¼ºæ¨¡å¼å¯¹æ¯”
======================================================================

æŒ‰å¢å¼ºæ¨¡å¼ç»Ÿè®¡:
                    best_val_f1                    overfit_gap          final_test_f1       
                           mean      std    max count        mean    std          mean   max
augmentation_mode                                                                            
none                     0.5630  0.0000 0.5630     1      0.3720 0.0000        0.5500 0.5500
light                    0.6050  0.0200 0.6150     2      0.2800 0.0300        0.5980 0.6050
medium                   0.6500  0.0000 0.6500     1      0.1300 0.0000        0.6450 0.6450
heavy                    0.6800  0.0000 0.6800     1      0.0400 0.0000        0.6750 0.6750


æ¯ç§æ¨¡å¼çš„å®éªŒ:

NONEæ¨¡å¼ (1ä¸ªå®éªŒ):
  - exp_20251122_120000: Val F1=0.5630, Gap=0.3720

LIGHTæ¨¡å¼ (2ä¸ªå®éªŒ):
  - exp_20251122_130000: Val F1=0.6150, Gap=0.2500
  - exp_20251122_125000: Val F1=0.5950, Gap=0.3100

MEDIUMæ¨¡å¼ (1ä¸ªå®éªŒ):
  - exp_20251122_143000: Val F1=0.6500, Gap=0.1300

HEAVYæ¨¡å¼ (1ä¸ªå®éªŒ):
  - exp_20251122_150000: Val F1=0.6800, Gap=0.0400
======================================================================
```

### 5. ç”Ÿæˆå®Œæ•´å¯¹æ¯”æŠ¥å‘Š

```bash
python view_experiments.py --compare
```

ç”Ÿæˆæ–‡ä»¶ï¼š
- `experiment_logs/comparison_report.md` - Markdownæ ¼å¼æŠ¥å‘Š
- `experiment_logs/comparison_plots.png` - å¯è§†åŒ–å¯¹æ¯”å›¾

### 6. å¯¼å‡ºè®ºæ–‡è¡¨æ ¼

```bash
python view_experiments.py --export
```

ç”Ÿæˆæ–‡ä»¶ï¼š
- `experiment_logs/paper_table.tex` - LaTeXæ ¼å¼è¡¨æ ¼
- `experiment_logs/paper_table.csv` - Excelå¯ç”¨çš„CSV

---

## ğŸ“Š è®ºæ–‡æ’°å†™å·¥ä½œæµ

### Step 1: è¿è¡Œæ‰€æœ‰å®éªŒ

```bash
# Baseline
python train_12steps_dual_cached.py  # ä¿®æ”¹ä»£ç è®¾ç½® augmentation='none'

# Lightå¢å¼º
python train_12steps_dual_cached.py  # ä¿®æ”¹ä¸º augmentation='light'

# Mediumå¢å¼º
python train_12steps_dual_cached.py  # ä¿®æ”¹ä¸º augmentation='medium'

# Heavyå¢å¼º
python train_12steps_dual_cached.py  # ä¿®æ”¹ä¸º augmentation='heavy'
```

### Step 2: ç”Ÿæˆå¯¹æ¯”ææ–™

```bash
# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
python view_experiments.py --compare

# å¯¼å‡ºè®ºæ–‡è¡¨æ ¼
python view_experiments.py --export

# æŸ¥çœ‹æ•°æ®å¢å¼ºå¯¹æ¯”
python view_experiments.py --augmentation
```

### Step 3: æ•´ç†è®ºæ–‡ææ–™

åœ¨`experiment_logs/`ç›®å½•ä¸‹ä¼šæœ‰ï¼š

**è¡¨æ ¼**:
- `experiments.csv` - å®Œæ•´å®éªŒè®°å½•
- `paper_table.tex` - å¯ç›´æ¥ç”¨äºLaTeX
- `paper_table.csv` - å¯å¯¼å…¥Excelç¼–è¾‘

**æŠ¥å‘Š**:
- `comparison_report.md` - è¯¦ç»†å¯¹æ¯”åˆ†æ

**å›¾è¡¨**:
- `comparison_plots.png` - 6å¼ å­å›¾çš„å¯¹æ¯”å¯è§†åŒ–
  - Val F1å¯¹æ¯”
  - Train vs Val F1æ•£ç‚¹å›¾ï¼ˆè¿‡æ‹Ÿåˆæ£€æµ‹ï¼‰
  - æ•°æ®å¢å¼ºæ•ˆæœ
  - Dropoutå½±å“
  - è¿‡æ‹Ÿåˆå·®è·å¯¹æ¯”
  - å„ç±»åˆ«F1åˆ†å¸ƒ

### Step 4: å†™è®ºæ–‡

ä½¿ç”¨`EXPERIMENT_PROTOCOL.md`ä¸­çš„æ¨¡æ¿ï¼š

```markdown
## 4.3 Data Augmentation

æˆ‘ä»¬è®¾è®¡äº†å¤šç§æ•°æ®å¢å¼ºç­–ç•¥...

**è¡¨1ï¼šæ•°æ®å¢å¼ºæ•ˆæœå¯¹æ¯”**
ï¼ˆä» paper_table.tex å¤åˆ¶ï¼‰

å¦‚è¡¨1æ‰€ç¤ºï¼Œæ•°æ®å¢å¼ºæ˜¾è‘—æ”¹å–„äº†æ¨¡å‹æ³›åŒ–æ€§èƒ½ã€‚
ç›¸æ¯”Baselineï¼ˆVal F1=0.563ï¼‰ï¼ŒHeavyå¢å¼ºä½¿Val F1æå‡è‡³0.680ï¼ˆ+20.8%ï¼‰ï¼Œ
åŒæ—¶è¿‡æ‹Ÿåˆå·®è·ä»0.372é™è‡³0.040ï¼ˆ-89.2%ï¼‰ã€‚

**å›¾1ï¼šå®éªŒå¯¹æ¯”å¯è§†åŒ–**
ï¼ˆä½¿ç”¨ comparison_plots.pngï¼‰
```

---

## ğŸ” å¸¸ç”¨æŸ¥è¯¢

### æ‰¾å‡ºè¿‡æ‹Ÿåˆæœ€å°çš„å®éªŒ

```python
import pandas as pd
df = pd.read_csv('experiment_logs/experiments.csv')
best = df.loc[df['overfit_gap'].idxmin()]
print(f"æœ€ä½è¿‡æ‹Ÿåˆ: {best['experiment_id']}")
print(f"  Gap: {best['overfit_gap']:.4f}")
print(f"  Config: {best['augmentation_mode']}, Dropout={best['dropout']}")
```

### æ‰¾å‡ºæŸä¸ªé…ç½®çš„æ‰€æœ‰å®éªŒ

```python
df = pd.read_csv('experiment_logs/experiments.csv')
medium_exps = df[df['augmentation_mode'] == 'medium']
print(medium_exps[['experiment_id', 'best_val_f1', 'overfit_gap']])
```

### è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”

```python
df = pd.read_csv('experiment_logs/experiments.csv')
baseline = df[df['augmentation_mode'] == 'none'].iloc[0]
best = df.loc[df['best_val_f1'].idxmax()]

improvement = (best['best_val_f1'] - baseline['best_val_f1']) / baseline['best_val_f1'] * 100
print(f"Val F1 æå‡: {improvement:.2f}%")

gap_reduction = (baseline['overfit_gap'] - best['overfit_gap']) / baseline['overfit_gap'] * 100
print(f"è¿‡æ‹Ÿåˆé™ä½: {gap_reduction:.2f}%")
```

---

## ğŸ’¡ å®éªŒè®°å½•æœ€ä½³å®è·µ

### 1. æ¯æ¬¡æ”¹åŠ¨éƒ½è¦è®°å½•

æ— è®ºæ”¹åŠ¨å¤šå°ï¼Œéƒ½è¿è¡Œå®Œæ•´è®­ç»ƒå¹¶è®°å½•ã€‚åŒ…æ‹¬ï¼š
- âŒ å¤±è´¥çš„å®éªŒä¹Ÿè¦è®°å½•ï¼ˆç”¨notesè¯´æ˜é—®é¢˜ï¼‰
- âœ… å‚æ•°å¾®è°ƒçš„å®éªŒ
- âœ… Bugä¿®å¤åçš„é‡è·‘

### 2. ä½¿ç”¨æè¿°æ€§æ ‡ç­¾

```python
# å¥½çš„æ ‡ç­¾
tags = ['baseline', 'no_aug', 'initial']
tags = ['augmentation', 'heavy', 'final_model']
tags = ['ablation', 'no_vision']

# ä¸å¥½çš„æ ‡ç­¾
tags = ['test', 'exp1', 'new']
```

### 3. è¯¦ç»†è®°å½•å¤‡æ³¨

```python
notes = """
ä¿®æ”¹å†…å®¹:
- æ•°æ®å¢å¼ºä»mediumæ”¹ä¸ºheavy
- å¢åŠ äº†temporal_cutout

è§‚å¯Ÿ:
- æ”¶æ•›å˜æ…¢ï¼ˆepoch 15 â†’ 25ï¼‰
- Class 0æ€§èƒ½æ˜¾è‘—æå‡ï¼ˆ+15%ï¼‰

ä¸‹ä¸€æ­¥:
- å°è¯•ä»…å¯¹Class 0/1å¢å¼º
"""
```

### 4. å®šæœŸç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

å»ºè®®æ¯5ä¸ªå®éªŒç”Ÿæˆä¸€æ¬¡æŠ¥å‘Šï¼š
```bash
python view_experiments.py --compare
```

### 5. å¤‡ä»½å®éªŒè®°å½•

```bash
# å®šæœŸå¤‡ä»½
cp -r experiment_logs experiment_logs_backup_20251122

# æˆ–ä½¿ç”¨git
git add experiment_logs/*.csv experiment_logs/*.json
git commit -m "å®éªŒè®°å½•ï¼šæµ‹è¯•heavyå¢å¼º"
```

---

## ğŸ“ ç›®å½•ç»“æ„

```
experiments/classification/
â”œâ”€â”€ experiment_tracker.py          # è¿½è¸ªç³»ç»Ÿæ ¸å¿ƒ
â”œâ”€â”€ view_experiments.py            # æŸ¥çœ‹è„šæœ¬
â”œâ”€â”€ EXPERIMENT_PROTOCOL.md         # è®°å½•è§„èŒƒ
â”œâ”€â”€ å®éªŒè®°å½•ä½¿ç”¨æŒ‡å—.md            # æœ¬æ–‡ä»¶
â”‚
â”œâ”€â”€ experiment_logs/               # å®éªŒè®°å½•ç›®å½•
â”‚   â”œâ”€â”€ experiments.csv           # ä¸»è®°å½•è¡¨ â­
â”‚   â”œâ”€â”€ exp_*_detail.json         # è¯¦ç»†é…ç½®
â”‚   â”œâ”€â”€ comparison_report.md      # å¯¹æ¯”æŠ¥å‘Š
â”‚   â”œâ”€â”€ comparison_plots.png      # å¯¹æ¯”å›¾è¡¨
â”‚   â”œâ”€â”€ paper_table.tex           # LaTeXè¡¨æ ¼
â”‚   â””â”€â”€ paper_table.csv           # CSVè¡¨æ ¼
â”‚
â”œâ”€â”€ checkpoints/                   # æ¨¡å‹checkpoints
â”‚   â””â”€â”€ timesclip_12steps_dual_*/
â”‚       â”œâ”€â”€ best_model.pth
â”‚       â”œâ”€â”€ config.json
â”‚       â””â”€â”€ training_log.txt
â”‚
â””â”€â”€ paper_materials/               # è®ºæ–‡ææ–™ï¼ˆå¯é€‰åˆ›å»ºï¼‰
    â”œâ”€â”€ figures/
    â””â”€â”€ tables/
```

---

## âš¡ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

| ä»»åŠ¡ | å‘½ä»¤ |
|------|------|
| è®­ç»ƒå¹¶è‡ªåŠ¨è®°å½• | `python train_12steps_dual_cached.py` |
| æŸ¥çœ‹æ‘˜è¦ | `python view_experiments.py` |
| æŸ¥çœ‹Top 5 | `python view_experiments.py --top 5` |
| æŸ¥çœ‹æ‰€æœ‰å®éªŒ | `python view_experiments.py --all` |
| å¯¹æ¯”æ•°æ®å¢å¼º | `python view_experiments.py --augmentation` |
| æŸ¥çœ‹å®éªŒè¯¦æƒ… | `python view_experiments.py --detail exp_XXX` |
| ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š | `python view_experiments.py --compare` |
| å¯¼å‡ºè®ºæ–‡è¡¨æ ¼ | `python view_experiments.py --export` |

---

## ğŸ“ è®ºæ–‡å†™ä½œæ¨¡æ¿

### å®éªŒéƒ¨åˆ†

```markdown
## 4. Experiments and Results

### 4.1 Experimental Setup

We evaluate our dual-modal TimesCLIP classifier on the 2018four dataset.

**Dataset**: 5557 samples, 14 variates, 37 time steps, 4 classes.
**Early recognition task**: Use only first 12 time steps (120 days).
**Data split**: Train 4000, Val 445, Test 1112 (stratified).

**Evaluation metrics**: Macro F1-score (primary), Accuracy.

### 4.2 Baseline Performance

Initial experiments without data augmentation showed severe overfitting:

| Model | Train F1 | Val F1 | Gap |
|-------|----------|--------|-----|
| Baseline | 0.935 | 0.563 | 0.372 |

ï¼ˆä½¿ç”¨ experiment_logs/paper_table.tex ä¸­çš„æ•°æ®ï¼‰

### 4.3 Data Augmentation

We designed a multi-modal augmentation strategy:

**Time-series augmentation** (Language branch):
- Gaussian noise (Ïƒ=0.02)
- Random scaling (0.9-1.1Ã—)
- Temporal shift (Â±2 steps)

**Image augmentation** (Vision branch):
- Random brightness (Â±10%)
- Random contrast (Â±10%)

**Table X: Augmentation Impact**

| Mode | Val F1 | Test F1 | Overfit Gap | Improvement |
|------|--------|---------|-------------|-------------|
| None | 0.563 | 0.550 | 0.372 | Baseline |
| Light | 0.605 | 0.598 | 0.280 | +7.5% |
| Medium | 0.650 | 0.645 | 0.130 | +15.5% |
| Heavy | 0.680 | 0.675 | 0.040 | **+20.8%** |

ï¼ˆæ•°æ®æ¥è‡ª python view_experiments.py --augmentationï¼‰

As shown in Table X, data augmentation significantly improved generalization.
The heavy augmentation achieved Val F1 of 0.680, a 20.8% improvement over 
baseline, while reducing overfitting gap by 89.2% (0.372â†’0.040).

**Figure X: Training Dynamics**
ï¼ˆä½¿ç”¨ comparison_plots.pngï¼‰

### 4.4 Ablation Study

ï¼ˆæŒ‰ç…§ EXPERIMENT_PROTOCOL.md ä¸­çš„æ¶ˆèå®éªŒæ¨¡æ¿ç¼–å†™ï¼‰

### 4.5 Final Results

Our final model (Heavy augmentation + Dropout 0.3) achieves:
- **Test F1**: 0.675
- **Test Accuracy**: 0.680
- **Per-class F1**: [0.60, 0.68, 0.72, 0.75]

ï¼ˆæ•°æ®æ¥è‡ªæœ€ä½³å®éªŒçš„test_results.jsonï¼‰
```

---

## ğŸ†˜ å¸¸è§é—®é¢˜

**Q: è®­ç»ƒå®Œæˆåæ²¡æœ‰è‡ªåŠ¨è®°å½•ï¼Ÿ**
A: æ£€æŸ¥æ˜¯å¦æœ€æ–°ç‰ˆæœ¬çš„`train_12steps_dual_cached.py`ï¼Œåº”è¯¥åŒ…å«`ExperimentTracker`ç›¸å…³ä»£ç ã€‚

**Q: experiments.csvæ‰¾ä¸åˆ°ï¼Ÿ**
A: ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨åˆ›å»ºã€‚å¦‚æœåˆ é™¤äº†ï¼Œè¿è¡Œ`python -c "from experiment_tracker import ExperimentTracker; ExperimentTracker()"`é‡æ–°åˆå§‹åŒ–ã€‚

**Q: å¦‚ä½•åˆ é™¤æŸä¸ªå®éªŒè®°å½•ï¼Ÿ**
A: æ‰‹åŠ¨ç¼–è¾‘`experiment_logs/experiments.csv`ï¼Œåˆ é™¤å¯¹åº”è¡Œï¼ŒåŒæ—¶åˆ é™¤`exp_XXX_detail.json`æ–‡ä»¶ã€‚

**Q: å¦‚ä½•æ¯”è¾ƒç‰¹å®šçš„å‡ ä¸ªå®éªŒï¼Ÿ**
A: ä½¿ç”¨Pythonï¼š
```python
from experiment_tracker import ExperimentTracker
tracker = ExperimentTracker()
tracker.compare_experiments(exp_ids=['exp_001', 'exp_002', 'exp_003'])
```

---

## âœ… æ£€æŸ¥æ¸…å•ï¼ˆè®ºæ–‡æäº¤å‰ï¼‰

- [ ] æ‰€æœ‰å®éªŒéƒ½å·²è®°å½•åˆ°experiments.csv
- [ ] ç”Ÿæˆäº†comparison_report.md
- [ ] å¯¼å‡ºäº†paper_table.texå’Œpaper_table.csv
- [ ] ä¿å­˜äº†comparison_plots.png
- [ ] æ¯ä¸ªå…³é”®å®éªŒéƒ½æœ‰è¯¦ç»†çš„notes
- [ ] Baselineå’Œæœ€ä½³æ¨¡å‹éƒ½æœ‰å®Œæ•´çš„test_results.json
- [ ] æ··æ·†çŸ©é˜µå·²ä¿å­˜
- [ ] è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜
- [ ] æ¶ˆèå®éªŒå®Œæ•´è®°å½•
- [ ] å¤‡ä»½äº†experiment_logsç›®å½•

---

**ç¥é¡ºåˆ©å®Œæˆè®ºæ–‡ï¼** ğŸ‰

